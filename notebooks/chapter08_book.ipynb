{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder via pytorch compared with ipca and t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis for linear reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation from the python module sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def towdir(s):\n",
    "    return (str('./datasets_book/'+s))\n",
    "\n",
    "import deepglmlib.utils as utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a dataset with clusters in 3d\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [\"blue\",\"red\",\"green\",\"orange\",\"purple\",\"brown\",\"olive\",\"magenta\",\"cyan\", \"black\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.load(towdir('x_y_10d_ae.npy'))\n",
    "x = xy[\"x\"]\n",
    "y = xy[\"y\"]\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a three dimensional view, the data are as follows when showing the three first components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "UPCA = PCA(n_components=2)\n",
    "z_pca = UPCA.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = title=\"Projection of 3d dataset with pca\"\n",
    "utils.f_plot_scatter(z_pca, y, title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "IPCA   = IncrementalPCA(n_components=2, batch_size=10)\n",
    "z_ipca = IPCA.fit_transform(x)\n",
    "title  = \"Projection of 3d dataset with ipca\"\n",
    "utils.f_plot_scatter(np.vstack([-z_ipca[:,0], -z_ipca[:,1]]).T, y, title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel PCA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "KPCA   = KernelPCA(n_components=2, kernel='sigmoid')\n",
    "# KPCA   = KernelPCA(n_components=2, kernel='rbf', alpha=0.3)\n",
    "z_kpca = KPCA.fit_transform(x)\n",
    "title  = \"Projection of 3d dataset with kpca\"\n",
    "utils.f_plot_scatter(z_kpca, y , title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-sne with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "z_tsne = TSNE(n_components=2, init='pca').fit_transform(x)\n",
    "title  = \"Projection of 3d dataset with tsne\"\n",
    "utils.f_plot_scatter(z_tsne, y ,title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality indicators for comparing the visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_score_projection(z,y,name=\"\",show=False):\n",
    "    y = y.ravel()\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import davies_bouldin_score\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    db = davies_bouldin_score(z, y)\n",
    "    sl = silhouette_score(z, y)\n",
    "    if show is True:\n",
    "        print(\"Davies_Bouldin_score of\",name, \"=\",np.round(db,3), \n",
    "              \"\\nSilhouette_score     of\",name, \"=\",np.round(sl,3))\n",
    "    return db, sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_pca, sl_pca = f_score_projection(z_pca,y,\"pca\",False)\n",
    "db_ipca, sl_ipca = f_score_projection(z_ipca,y,\"ipca\",False)\n",
    "db_kpca, sl_kpca = f_score_projection(z_kpca,y,\"kpca\",False)\n",
    "db_tsne, sl_tsne = f_score_projection(z_tsne,y,\"tsne\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders  for linear and nonlinear reduction with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with the artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, name, layers_encoder, layers_decoder, init_layers = None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.layers_encoder = layers_encoder\n",
    "        self.layers_decoder = layers_decoder\n",
    "        self.net_encoder = nn.Sequential(*layers_encoder)\n",
    "        self.net_decoder = nn.Sequential(*layers_decoder)\n",
    "        self.init_layers = init_layers\n",
    "        if self.init_layers is not None:\n",
    "            for k in self.init_layers:\n",
    "                torch.nn.init.xavier_uniform_(self.net_encoder[k].weight)\n",
    "                torch.nn.init.xavier_uniform_(self.net_decoder[k].weight)        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.net_encoder(x)\n",
    "        decoded = self.net_decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encoder(self,x):\n",
    "        encoded = self.net_encoder(x)\n",
    "        return encoded\n",
    "    \n",
    "    def decoder(self,z):\n",
    "        decoded = self.net_decoder(z)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import deepglmlib.utils as utils\n",
    "\n",
    "dataset = torch.utils.data .TensorDataset( torch.from_numpy(x).float(), torch.from_numpy(y).int() )\n",
    "\n",
    "dl_train, dl_test, n, n_train, n_test = utils.f_splitDataset(dataset)  # batch size ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "layers_encoder = []\n",
    "layers_encoder.append(nn.Linear(x.shape[1],7, bias=True))\n",
    "layers_encoder.append(nn.Linear(7, 4))\n",
    "layers_encoder.append(nn.Linear(4, 2))\n",
    "\n",
    "layers_decoder = []\n",
    "layers_decoder.append(nn.Linear(2, 4))\n",
    "layers_decoder.append(nn.Linear(4, 7))\n",
    "layers_decoder.append(nn.Linear(7,x.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f_train_autoencoder(dl_train,autoencoder,nbmax_epoqs,lr,device=None,epoch_print=5):\n",
    "    optimizer = torch.optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    loss = nn.MSELoss(reduction='sum')\n",
    "    loss_s = np.zeros(nbmax_epoqs)\n",
    "    if device is not None: autoencoder=autoencoder.to(device)\n",
    "    autoencoder.train()\n",
    "    t=0\n",
    "    for epoch in range(nbmax_epoqs):\n",
    "        loss_t = 0 \n",
    "        for step, tuple_b in enumerate(dl_train):\n",
    "            xb = tuple_b[0]\n",
    "            yb = tuple_b[1]\n",
    "            if device is not None:\n",
    "                xb=xb.to(device)\n",
    "                yb=yb.to(device)\n",
    "            xb_hat = autoencoder(xb)\n",
    "            lossb = loss(xb_hat, xb)       \n",
    "            optimizer.zero_grad()               \n",
    "            lossb.backward()                     \n",
    "            optimizer.step()\n",
    "            loss_t += lossb\n",
    "        loss_s[t] = loss_t\n",
    "        if epoch % epoch_print == 0 or (epoch == nbmax_epoqs-1 and epoch_print<=nbmax_epoqs):\n",
    "            print(\"t=\",t,\"\\tloss=\",np.round(loss_t.detach().cpu().numpy(),5))\n",
    "        t+=1\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    tmax = t\n",
    "    return loss_s, tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder =  AutoEncoder(\"AE-3-2\",copy.deepcopy(layers_encoder),copy.deepcopy(layers_decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_s, tmax = f_train_autoencoder(dl_train,autoencoder,1000,0.0001,epoch_print=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_ae = autoencoder.encoder(torch.from_numpy(x).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_ae.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = \"Projection of 3d dataset with ae\"\n",
    "utils.f_plot_scatter(z_ae, y, title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_ae, sl_ae = f_score_projection(z_ae,y,\"ae\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear autoencoder and training\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers_encoder = []\n",
    "layers_encoder.append(nn.Linear(x.shape[1],7, bias=True))\n",
    "layers_encoder.append(nn.Tanh())\n",
    "layers_encoder.append(nn.Linear(7, 4))\n",
    "layers_encoder.append(nn.Tanh())\n",
    "layers_encoder.append(nn.Linear(4, 2))\n",
    "\n",
    "layers_decoder = []\n",
    "layers_decoder.append(nn.Linear(2, 4))\n",
    "layers_decoder.append(nn.Tanh())\n",
    "layers_decoder.append(nn.Linear(4, 7))\n",
    "layers_decoder.append(nn.Tanh())\n",
    "layers_decoder.append(nn.Linear(7,x.shape[1]))\n",
    "\n",
    "autoencoder_nl =  AutoEncoder(\"AE-\",copy.deepcopy(layers_encoder),copy.deepcopy(layers_decoder), [0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_nl_s, tmax_nl = f_train_autoencoder(dl_train,autoencoder_nl,1500,0.001,epoch_print=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_ae_nl = autoencoder_nl.encoder(torch.from_numpy(x).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.title = \"Projection of 3d dataset with nonlinear ae\"\n",
    "utils.f_plot_scatter(z_ae_nl, y, title=title, isellipse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_ae_nl, sl_ae_nl = f_score_projection(z_ae_nl,y,\"ae_nl\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of different visualisations with two indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "method_s = [\"pca (400)\", \"ipca (400)\", \"kpca (400)\", \"t-sne (400)\", \n",
    "            \"linear ae (400)\", \"non linear ae (400)\"]\n",
    "\n",
    "db_s = [db_pca, db_ipca, db_kpca, db_tsne, db_ae, db_ae_nl]\n",
    "\n",
    "sl_s = [sl_pca, sl_ipca, sl_kpca, sl_tsne, sl_ae, sl_ae_nl]\n",
    "\n",
    "results = [method_s, db_s, sl_s]\n",
    "results_pd = pd.DataFrame(results).transpose()\n",
    "results_pd.columns = [\"method (sample size)\", \"davis-bouldin\", \"silhouettes\"]\n",
    "\n",
    "with pd.option_context('float_format', '{:.4f}'.format, 'display.expand_frame_repr', False):\n",
    "    print(results_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder and t-sne with a dataset of 60000 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset (from the image files to one binary hdf5 file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt#; plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "dir_mnist = towdir(\"\")\n",
    "\n",
    "dl_train = DataLoader( MNIST(dir_mnist, train=True, download=True,\n",
    "                             transform=torchvision.transforms.ToTensor()),\n",
    "                       batch_size=200, shuffle=True)\n",
    "\n",
    "dl_test  = DataLoader( MNIST(dir_mnist, train=False, download=True,\n",
    "                             transform=torchvision.transforms.ToTensor()),\n",
    "                       batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.f_save_dataloader_to_h5py(dl_train,towdir(\"mnist60000.h5\"),28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_mnist = utils.DatasetH5(towdir('mnist60000.h5'),'x','y') \n",
    "print(dataset_mnist.x.shape, dataset_mnist.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl_train, dl_test, n, n_train, n_test = utils.f_splitDataset(dataset_mnist,0.8,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(n,n_train,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result from t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of file processing with a random projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_all = DataLoader(dataset_mnist,shuffle=False,batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_x = towdir('x_mnist60000.memmap')\n",
    "filename_y = towdir('y_mnist60000.memmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = dataset_mnist.x.shape[0]\n",
    "p = dataset_mnist.x.shape[1]\n",
    "\n",
    "utils.f_save_dl_xy_to_2memmap(dl_all, filename_x, filename_y,\n",
    "                              n=n, p=p, is_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_s = np.random.binomial(size=n,p=0.05,n=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_read_memmap(filename_x,n,p):\n",
    "    x_map = np.memmap(filename_x, dtype='float32', \n",
    "                      mode='r', shape=(n,p))\n",
    "    return x_map\n",
    "\n",
    "def f_write_memmap(filename_x,n,p):\n",
    "    x_map = np.memmap(filename_x, dtype='float32', \n",
    "                      mode='w+', shape=(n,p))\n",
    "    return x_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import random_projection\n",
    "\n",
    "for tried in range(1,10):\n",
    "    print(\"---------------------------------------------\")\n",
    "    print(\"tried=\",tried,\"/10\")\n",
    "    \n",
    "    n, p = (60000, 784)\n",
    "    x_map = f_read_memmap(filename_x,n,p)\n",
    "    y_map = f_read_memmap(filename_y,n,1)    \n",
    "    \n",
    "    print(\"compute random projection\")\n",
    "    GaussianRP          = random_projection.GaussianRandomProjection\n",
    "    p_random            = 120\n",
    "    mapper              = GaussianRP(n_components=p_random)\n",
    "    x_map_mnist_reduced = mapper.fit_transform(x_map[0:100,:])\n",
    "    RR                  = mapper.components_\n",
    "    np.savetxt(\"./datasets_book/RR_120_784_mnist60000.txt\",RR)\n",
    "\n",
    "    RR = np.loadtxt(towdir(\"RR_120_784_mnist60000.txt\"))\n",
    "\n",
    "    filename_xin  = towdir('x_mnist60000.memmap')\n",
    "    filename_xout = towdir(\"x_rp120_mnist60000.memmap\")\n",
    "\n",
    "    utils.f_save_to_reduction_to_memmap_files(filename_xin,\n",
    "                                        filename_xout,\n",
    "                                        R=RR.transpose(),\n",
    "                                        n = x_map.shape[0],\n",
    "                                        size_minibatch = 250)\n",
    "\n",
    "    filename_z = towdir(\"x_rp120_mnist60000.memmap\")\n",
    "\n",
    "    n = 60000\n",
    "    p = 120\n",
    "\n",
    "    z_rp_mnist = f_read_memmap(filename_z,n,p)\n",
    "    y_mnist   = y_map\n",
    "\n",
    "    z_rp_mnist.shape, y_mnist.shape\n",
    "\n",
    "    mn = np.sum(z_rp_mnist,axis=0)          # by chunks?\n",
    "    sd = np.sqrt(np.var(z_rp_mnist,axis=0)) # by chunks?\n",
    "\n",
    "    filename_zout = towdir(\"x_rp120_mnist6000_standardized.memmap\")\n",
    "\n",
    "    z_rp_mnist_strd = np.memmap(filename_zout, \n",
    "                                dtype='float32', mode='w+', \n",
    "                                shape=(n,120))\n",
    "\n",
    "    size_chunks = 100\n",
    "\n",
    "    for idx_b in range(0, n, size_chunks):\n",
    "        idx_b2 = np.min( [idx_b+size_chunks,n] )\n",
    "        zb                    = z_rp_mnist[idx_b:idx_b2,:]\n",
    "        z_rp_mnist_strd[idx_b:idx_b2,:] = (zb - mn)/sd\n",
    "\n",
    "    del z_rp_mnist_strd\n",
    "\n",
    "\n",
    "\n",
    "    size_chunks = 100\n",
    "    n_components = 50\n",
    "\n",
    "\n",
    "    z_rp_mnist_strd = \\\n",
    "      f_read_memmap( towdir(\"x_rp120_mnist6000_standardized.memmap\"), \n",
    "                     n,120 )\n",
    "\n",
    "    z_ipca_150rp    = \\\n",
    "       f_write_memmap( towdir(\"z_ipca50_150rp_mnist6000.memmap\"),\n",
    "                       n,n_components )\n",
    "\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    ipca = IncrementalPCA( n_components= n_components, \n",
    "                           batch_size= size_chunks )\n",
    "\n",
    "    print(\"compute transformation pca\")\n",
    "    # for epoch in range(5):\n",
    "    for idx_b in range(0, n, size_chunks):\n",
    "        idx_b2 = np.min( [idx_b+size_chunks,n] )\n",
    "        ipca.partial_fit(z_rp_mnist_strd[idx_b:idx_b2,:])\n",
    "\n",
    "    print(\"compute reduced coordinates\")\n",
    "    for idx_b in range(0, n, size_chunks):\n",
    "        idx_b2 = np.min( [idx_b+size_chunks,n] )\n",
    "        z_ipca_150rp[idx_b:idx_b2,:] = \\\n",
    "          ipca.transform(z_rp_mnist_strd[idx_b:idx_b2,:])\n",
    "\n",
    "    del z_rp_mnist_strd, z_ipca_150rp\n",
    "    \n",
    "    print(\"compute t-sne mapping\")\n",
    "    z_ipca_150rp    = \\\n",
    "       f_read_memmap( towdir(\"z_ipca50_150rp_mnist6000.memmap\"),\n",
    "                       60000,50 )\n",
    "\n",
    "\n",
    "    x_map_init = copy.deepcopy( z_ipca_150rp[:,0:2] )\n",
    "    x_map = z_ipca_150rp\n",
    "\n",
    "    z_mnist_tsne_2, z_init_mnist_tsne_2, aff_mnist_2 = \\\n",
    "        utils.f_projection_from_openTSNE(x_map = x_map,\n",
    "                                   x_map_init=x_map_init,\n",
    "                                   perplexity=30,n_jobs=3,\n",
    "                                   random_state=0,verbose=False)\n",
    "\n",
    "    np.savetxt(\"./datasets_book/z_mnist_tsne_2\"+\"_tried\"+str(tried)+\".txt\",z_mnist_tsne_2)\n",
    "    np.savetxt(\"./datasets_book/y_mnist_2.txt\",y_mnist)\n",
    "    y_mnist_2      = y_mnist\n",
    "\n",
    "    title=\"Projection of mnist dataset with tnse after rand-proj\"\n",
    "    utils.f_plot_scatter(z_mnist_tsne_2, y_mnist, title=title, isellipse=True)\n",
    "\n",
    "    print(\"compute t-sne quality\")\n",
    "    db_tsne_mnist_2, sl_tsne_mnist_2 = \\\n",
    "       f_score_projection(z_mnist_tsne_2[i_s==1,:],\n",
    "       y_mnist[i_s==1].ravel(),\n",
    "       \"tsne-mnist-after-rand-proj\",False)\n",
    "\n",
    "    print(np.round(db_tsne_mnist_2,3),\n",
    "          np.round(sl_tsne_mnist_2,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
