## Data files and companion jupyter notebooks for the book: <br />

## "Linear and Deep Models Basics with Pytorch, Numpy, and Scikit-Learn" <br />
 <br />
 <br />

**Main Document (247 pages) without the chapter for the exercices** [book_rpriam_2022.pdf](https://github.com/rpriam/book1/blob/main/book_rpriam_2022.pdf)

**Printed book with corrected exercices is available at amazon.com** [link_to_the_website.html](https://www.amazon.com/dp/B0BRDGQND1) <br />

[![Cover book](https://github.com/rpriam/book1/blob/main/cover.png)](https://www.amazon.com/dp/B0BRDGQND1) 

**Main Features** <br />

- Theory for the linear models and implementation with pytorch and scikit-learn  <br />

- Practice of deep learning with pytorch for feedforward neural networks <br />

- Many examples and exercices to practice and understand better the contents <br />

- Very large datasets 450000 and 11000000 on a home computer with a few gigabytes  <br />

- Step by step for theory & code (require only minimum knowledge in python and maths)  <br />

**Abstract**  <br />

This book is an introduction to computational statistics for the generalized linear models (glm) and to machine learning with the python language. Extensions of the glm with nonlinearities come from hidden layer(s) within a neural network for linear and nonlinear regression or classification. This allows to present side by side classical statistics and current deep learning. The loglikelihoods and the corresponding loss functions are explained. The gradient and hessian matrix are discussed and implemented for these linear and nonlinear models. Several methods are implemented from scratch with numpy for prediction (linear, logistic, poisson regressions) and for reduction (principal component analysis, random projection). The gradient descent, newton-raphson, natural gradient and l-fbgs algorithms are implemented. The datasets in stake are with 10 to 10^7 rows, and are tabular such that images or texts are vectorized. The data are stored in a compressed format (memmap or hdf5) and loaded by chunks for several case studies with pytorch or scikit-learn. Pytorch is presented for training with minibatches via a generic implementation for studying with computer programs. Scikit-learn is presented for processing large datasets via the partial fit, after the small examples. Sixty exercises are proposed at the end of the chapters with selected solutions to go beyond the contents. <br />


**Chapter** <br />

1. Introduction <br />

Polynomial regression  <br />
Error on a train sample  <br />
Error on a test sample  <br />

3. Linear models with numpy and scikit-learn <br />

Theory for the linear regression  <br />
Theory for logistic regression <br />
Loglikelihood and loss function <br />
Derivatives, implementation with numpy <br />
Implementation with Scikit-Learn <br />

5. First-order training of linear models <br />

Algorithm with one datum and with one minibatch <br />
Implementation of the algorithms with numpy <br />
Implementation of the algorithms with pytorch <br />

7. Neural networks for (deep) glm <br />
8. Lasso selection for (deep) glm <br />
9. Hessian and covariance for (deep) glm <br />
10. Second-order training of (deep) glm <br />
11. Autoencoder compared to ipca and t-sne <br />
12. Solution to selected exercices <br />



**Details from Amazon kdp (December 27, 2022)** <br />

ASIN : B0BRDGQND1 <br />
Language : English <br />
Paperback : 283 pages <br />
ISBN-13 : 979-8371441577 <br />







